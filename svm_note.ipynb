{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Introduction to Support Vector Machine\n",
    "\n",
    "- Hard-margin SVM\n",
    "  - Two-class linear classification\n",
    "  - Calculation of margin\n",
    "  - Langulange multiplier\n",
    "  - KKT condition\n",
    "  - Support vectors\n",
    "  - SMO algorithm\n",
    "- Soft-margin SVM\n",
    "  - Slack variable\n",
    "  - Penalty C\n",
    "- Nonlinear SVM\n",
    "  - Kernel function\n",
    "  - Kernel trick\n",
    "- Appendix\n",
    "  - Example of Langulange multipler\n",
    "  - Quadratic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-margin SVM\n",
    "\n",
    "### Two-class linear classification\n",
    "\n",
    "We will derive the concept of support vector machine from the two-class classification problem using linear models of the form \n",
    "\n",
    "\\begin{equation}\n",
    "y(\\bf{x}) = \\bf{w}^T \\bf{x} + b\n",
    "\\end{equation}\n",
    "\n",
    " where $\\bf{w}$ is a weight vector and $b$ is a bias parameter.\n",
    "\n",
    "The training data set comprises $N$ input vectors $x_1,..., x_N$ , with\n",
    "corresponding target values $t_1,...,t_N$ where $t_n ∈ \\{−1, 1\\}$, and new data points $x$ are classified according to the sign of $y(x)$.\n",
    "\n",
    "\n",
    "We shall assume for the moment that the training data set is linearly separable in\n",
    "feature space, so that by definition there exists at least one choice of the parameters\n",
    "$\\bf{w}$ and $b$ such that a function of the form (7.1) satisfies $y(x_n) > 0$ for points having\n",
    "$t_n = +1$ and $y(x_n) < 0$ for points having $t_n = −1$, so that $t_n y(x_n) > 0$ for all training data points."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of margin\n",
    "In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. For this purpose, we need to calculate the distance of a point $\\bf{x}_m$ to the decision surface (hyperplane)\n",
    "\n",
    "Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$ both of which lie on the decision surface.\n",
    "Because $y(\\mathbf{x}_A) = y(\\mathbf{x}_B) = 0$, we have $\\mathbf{w}^T (\\mathbf{x}_A − \\mathbf{x}_B)=0$ and hence the vector $\\mathbf{w}$ is\n",
    "orthogonal to every vector lying within the decision surface, and so $\\mathbf{w}$ determines the orientation of the decision surface. Then the point on the phyperplace $\\mathbf{x}'$ that is the closest point to the origin can be represented as $\\mathbf{x}' = aw$ where $a$ is a scalar value. Since $\\mathbf{x}'$ is on the hyperplane, we know that $w^T \\mathbf{x}' + w_0 \\Rightarrow a w^T w + w_0 = 0 \\Rightarrow a = \\frac{-w_0}{||w||^2}$. Therefore, $||\\mathbf{x}'|| = ||aw|| = -\\frac{-w_0}{||w||}$\n",
    "\n",
    "<img width=400 src=\"https://i.stack.imgur.com/tnAsd.png\">\n",
    "\n",
    "Now, the perpendicular distance of a point $\\bf{x}$ from a hyperplane defined by $y(\\mathbf{x})=0$ is given by $|y(x)|/||w||$.\n",
    "\n",
    "tny(xn)\n",
    "\u0005w\u0005 = tn(wTφ(xn) + b)\n",
    "\u0005w\u0005 . (7.2)\n",
    "The margin is given by the perpendicular distance to the closest point xn from the\n",
    "data set, and we wish to optimize the parameters w and b in order to maximize this\n",
    "distance. Thus the maximum margin solution is found by solving\n",
    "arg max w,b \f",
    " 1\n",
    "\u0005w\u0005 min\n",
    "n\n",
    "\b\n",
    "tn\n",
    "\n",
    "\n",
    "wTφ(xn) + b\n",
    "\u000b",
    "\t\n",
    "    \n",
    "The optimization problem then simply requires that we maximize \u0005w\u0005−1, which is\n",
    "equivalent to minimizing \u0005w\u00052, and so we have to solve the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{argmin}}\n",
    "\\frac{1}{2}||w||^2\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langulange multiplier\n",
    "In order to solve this constrained optimization problem, we introduce Lagrange multipliers $a_n \\ge 0$, with one multiplier an for each of the constraints, giving\n",
    "the Lagrangian function\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{w}, b, \\mathbf{a}) = \\frac{1}{2}||\\mathbf{w}||^2 − \n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n {t_n(\\mathbf{w}^T \\mathbf{x}_n + b) − 1}\n",
    "\\end{equation}\n",
    "\n",
    "Setting the derivatives of $L$ with respect to $\\bf{w}$ and $b$ equal to zero, we obtain the following two conditions\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \\overset{N}{\\underset{n=1}{\\sum}} a_n t_n \\mathbf{x}_n\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n t_n = 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Eliminating $\\bf{w}$ and $b$ from $L(\\mathbf{w}, b, \\mathbf{a})$ using these conditions then gives **the dual representation** of the maximum margin problem in which we want to maximize\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m \\mathbf{x}_n \\mathbf{x}_m\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KKT condition\n",
    "\n",
    "Because the above Laglangue multiplier was an optimazation of a function with inequality constraints, it has to satisfy the following condition, known as KTT condition (Karush–Kuhn–Tucker conditions).\n",
    "\n",
    "\\begin{equation}\n",
    "a_n \\ge 0 \\\\\n",
    "t_n y(x_n) − 1 \\ge 0 \\\\\n",
    "a_n {t_n y(x_n) − 1} = 0 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Here the idea of support vector comes in. For every data point, either $a_n = 0$ or $t_n y(x_n) = 1$. Any data point for\n",
    "which $a_n = 0$ will not appear in the sum in (7.13) and hence plays no role in making predictions for new data points. The remaining data points are called **support vectors**,\n",
    "and because they satisfy $t_n y(x_n) = 1$, they correspond to points that lie on the maximum margin hyperplanes in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-margin SVM\n",
    "\n",
    "### Slack variable\n",
    "\n",
    "### Penalty C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM\n",
    "\n",
    "### Kernel functions\n",
    "\n",
    "### Kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic programming\n",
    "The above optimization is an example of quadratic programming (QP). QP optimizes (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables. Intuitively, in case of N = 2, this is similar to find an optimal countor group of an ellipse/circle or other quadratic shape with several linear constraints. <img width=300 src=\"https://www.researchgate.net/profile/Christian_Bauckhage/publication/335099466/figure/fig1/AS:790344266444801@1565444165906/Contour-plot-of-the-objective-function-f-x-in-2-and-visualizations-of-the-two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referece\n",
    "- [SVM in SVG](https://shogo82148.github.io/homepage/memo/algorithm/svm/kernel-svm.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
