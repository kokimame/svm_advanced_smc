{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Introduction to Support Vector Machine\n",
    "\n",
    "- Hard-margin SVM\n",
    "  - Two-class linear classification\n",
    "  - Calculation of margin\n",
    "  - Langulange multiplier\n",
    "  - KKT condition\n",
    "  - Support vectors\n",
    "  - SMO algorithm\n",
    "- Soft-margin SVM\n",
    "  - Slack variable\n",
    "  - Penalty C\n",
    "- Nonlinear SVM\n",
    "  - Kernel function\n",
    "  - Kernel trick\n",
    "- Appendix\n",
    "  - Example of Langulange multipler\n",
    "  - Quadratic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of mathmatical fomulation decribed below comes from the reference [Bishop]. But I start with a simple linear kernel version with some extra notes to make them easy to understand.\n",
    "## Hard-margin SVM\n",
    "\n",
    "### Two-class linear classification\n",
    "\n",
    "We will derive the concept of support vector machine from the two-class classification using a linear model $y$ of the form \n",
    "\n",
    "\\begin{equation}\n",
    "y(\\bf{x}) = \\bf{w}^T \\bf{x} + b\n",
    "\\end{equation}\n",
    "\n",
    " where $\\bf{w}$ is a weight vector and $b$ is a bias parameter.\n",
    "\n",
    "The training data set comprises $N$ input vectors $\\mathbf{x}_1, ...,  \\mathbf{x}_N$ with\n",
    "corresponding target values $t_1,...,t_N$ where $t_n ∈ \\{−1, 1\\}$, and new data points $\\mathbf{x}$ are classified according to the sign of $y(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "We shall assume for the moment that **the training data set is linearly separable in\n",
    "feature space**, so that by definition there exists at least one choice of the parameters\n",
    "$\\mathbf{w}$ and $b$ such that a function of the form (7.1) satisfies $y(\\mathbf{x}_n) > 0$ for points having\n",
    "$t_n = +1$ and $y(\\mathbf{x}_n) < 0$ for points having $t_n = −1$, so that $t_n y(\\mathbf{x}_n) > 0$ for all training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of margin\n",
    "In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. For this purpose, we need to calculate the distance of a point $\\bf{x}_m$ to the decision surface (hyperplane)\n",
    "\n",
    "Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$ both of which lie on the decision surface.\n",
    "Because $y(\\mathbf{x}_A) = y(\\mathbf{x}_B) = 0$, we have $\\mathbf{w}^T (\\mathbf{x}_A − \\mathbf{x}_B)=0$ and hence the vector $\\mathbf{w}$ is\n",
    "orthogonal to every vector lying within the decision surface, and so $\\mathbf{w}$ determines the orientation of the decision surface. Then the point $\\mathbf{x}'$ on the hyperplane that is the closest point to the origin can be represented as $\\mathbf{x}' = a\\mathbf{w}$ where $a$ is a scalar value. Since $\\mathbf{x}'$ is on the hyperplane, we know that $\\mathbf{w}^T \\mathbf{x}' + b = 0\\Rightarrow a \\mathbf{w}^T \\mathbf{w} + b = 0 \\Rightarrow a = -\\frac{b}{||w||^2}$. Therefore, $||\\mathbf{x}'|| = ||aw|| = -\\frac{b}{||w||}$. Now rewrite $\\mathbf{x}'$ to $\\mathbf{x}$ which satisfy $y(\\mathbf{x}) = 0$, and the distance of a point $\\mathbf{x}$ from a hyperplane is given by $|y(\\mathbf{x})|/||w||$. Furthermore, we are only interested in solutions for which all data points are correctly classified, so that $t_n y(\\mathbf{x}_n) > 0$ for all $n$. Thus the distance of a point $\\mathbf{x}_n$ to the decision surface is given by \n",
    "\\begin{equation}\n",
    "\\frac{t_n y(\\mathbf{x}_n)}{||\\mathbf{w}||} = \\frac{t_n(\\mathbf{w}^T \\mathbf{x}_n + b)}{||\\mathbf{w}||}\n",
    "\\end{equation}\n",
    "\n",
    "<img width=400 src=\"https://i.stack.imgur.com/tnAsd.png\">\n",
    "\n",
    "The margin is given by the perpendicular distance to the closest point $\\mathbf{x}_n$ from the\n",
    "data set, and we wish to optimize the parameters $\\mathbf{w}$ and $b$ in order to maximize this\n",
    "distance. Thus the maximum margin solution is found by solving\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{argmax}} \\{ \\frac{1}{||\\mathbf{w}||}\n",
    "\\underset{n}{\\mathrm{argmin}} [ t_n(\\mathbf{w}^T \\mathbf{x}_n + b) ] \\}\n",
    "\\end{equation}\n",
    "We note that the margin $|y(\\mathbf{x})|/||w||$ is invariant to the rescaling $\\mathbf{w} \\rightarrow k\\mathbf{w}$ and $b \\rightarrow kb$, and we use this freedom to set \n",
    "\\begin{equation}\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) = 1\n",
    "\\end{equation}\n",
    "\n",
    "for the point that is closest to the surface. In this case, all data points ($n = 1, ..., N$) will satisfy the constraints\n",
    "\n",
    "\\begin{equation}\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) \\ge 1\n",
    "\\end{equation}\n",
    "\n",
    "The optimization problem then simply requires that we maximize $\\frac{1}{||\\mathbf{w}||}$, which is\n",
    "equivalent to minimizing $||\\mathbf{w}||^2$, and so we have to solve the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{argmin}}\n",
    "\\frac{1}{2}||\\mathbf{w}||^2\n",
    "\\end{equation}\n",
    "\n",
    "subject to the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langulange multiplier\n",
    "In order to solve this constrained optimization problem, we introduce Lagrange multipliers $a_n \\ge 0$, with one multiplier an for each of the constraints, giving\n",
    "the Lagrangian function\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{w}, b, \\mathbf{a}) = \\frac{1}{2}||\\mathbf{w}||^2 − \n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n {t_n(\\mathbf{w}^T \\mathbf{x}_n + b) − 1}\n",
    "\\end{equation}\n",
    "\n",
    "Setting the derivatives of $L$ with respect to $\\bf{w}$ and $b$ equal to zero, we obtain the following two conditions\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \\overset{N}{\\underset{n=1}{\\sum}} a_n t_n \\mathbf{x}_n\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n t_n = 0\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Eliminating $\\bf{w}$ and $b$ from $L(\\mathbf{w}, b, \\mathbf{a})$ using these conditions then gives the **dual representation** of the maximum margin problem in which we want to maximize\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m \\mathbf{x}_n \\mathbf{x}_m\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KKT condition\n",
    "\n",
    "Because the above Lagrange multiplier was an optimazation of a function with inequality constraints, it has to satisfy the following condition, known as KTT condition (Karush–Kuhn–Tucker conditions).\n",
    "\n",
    "\\begin{equation}\n",
    "a_n \\ge 0 \\\\\n",
    "t_n y(x_n) − 1 \\ge 0 \\\\\n",
    "a_n \\{t_n y(x_n) − 1\\} = 0 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Here the idea of support vector comes in. For every data point, either $a_n = 0$ or $t_n y(x_n) = 1$. Any data point for\n",
    "which $a_n = 0$ will not appear in the sum in (7.13) and hence plays no role in making predictions for new data points. The remaining data points are called **support vectors**,\n",
    "and because they satisfy $t_n y(x_n) = 1$, they correspond to points that lie on the maximum margin hyperplanes in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft-margin SVM\n",
    "\n",
    "### Slack variable\n",
    "\n",
    "### Penalty C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM\n",
    "\n",
    "### Kernel function\n",
    "So far what we saw is a linear kernel.\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m \\mathbf{x}_n \\mathbf{x}_m\n",
    "\\end{equation}\n",
    "\n",
    "### Kernel trick\n",
    "With kernel functions, you can operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the **kernel trick** and a class of algorithms which employs the technique is called kernel methods.\n",
    "\n",
    "- Without kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Lagrange multiplier\n",
    "For problems where you want to find the maximum value of $f(x, y)$, subject to $g(x, y)$, you can use the following approach,\n",
    "> Given $L(x, y, \\lambda) = f(x, y) - \\lambda g(x, y)$, if $(\\alpha, \\beta)$ gives an extreme value, it is a solution of $\\frac{\\delta L}{\\delta x} = \\frac{\\delta L}{\\delta y} = \\frac{\\delta L}{\\delta \\lambda} = 0$ or $\\frac{\\delta g}{\\delta x} = \\frac{\\delta g}{\\delta y} = 0$\n",
    "\n",
    "This is called Lagrange multiplier and $L$ is called Lagrange function. A simple example follows:\n",
    "\n",
    "> Find the maximum value of the objective function $f(x, y) = 2x + 3y$, subject to the constraint $x^2 + y^2 = 1$\n",
    "\n",
    "In this case, we have a constraint $g(x, y) = x^2 + y^2 - 1 = 0$.\n",
    "First, we make L with three variables.\n",
    "$L(x,y,\\lambda)=2x+3y−\\lambda(x^2+y^2−1)$\n",
    "\n",
    "Setting the derivatives of $L(x, y, \\lambda)$ with respect to each variable to zero\n",
    "zero, \n",
    "\\begin{equation}\n",
    "\\frac{\\delta L}{\\delta x} = 2 - 2x\\lambda = 0\\\\\n",
    "\\frac{\\delta L}{\\delta y} = 3 - 2y\\lambda = 0\\\\\n",
    "\\frac{\\delta L}{\\delta \\lambda} = -x^2 - y^2 + 1 = 0\n",
    "\\end{equation}\n",
    "\n",
    "we obtain $x = 23y$ by removing $\\lambda$ from the equations.\n",
    "Use this in the third equation, then the candidate positions of extreme values are\n",
    "$(x,y)=(\\pm \\frac{2}{\\sqrt{13}}, \\pm\\frac{3}{\\sqrt{13}})$ (See the code below)\n",
    "\n",
    "Great news is that you can apply Lagrange multiplier to any other multivariable functions than two ($x, y$). Also, multiple constraints can be dealt in the same way. If you have $g(x) = 0$, $h(x) = 0$, we obtain the Lagrange function $L = f - \\lambda g - \\mu h$. This is what we see in the SVM part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x119a27358>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kp = 2*2/13**0.5 + 3*3/13**0.5\n",
    "km = -2*2/13**0.5 - 3*3/13**0.5\n",
    "y = lambda x, k: -(2/3)*x + k/3\n",
    "xs = np.linspace(-2, 2, 50)\n",
    "circle = plt.Circle((0, 0), 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.add_artist(circle)\n",
    "ax.plot(xs, [y(x, kp) for x in xs], c=\"r\", label=\"max f(x, y)\")\n",
    "ax.plot(xs, [y(x, km) for x in xs], c=\"c\", label=\"min f(x, y)\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic programming\n",
    "Quadratic programming optimizes (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables. Intuitively, in case of N = 2, this is similar to find an optimal countor group of an ellipse/circle or other quadratic shape with several linear constraints. <img width=300 src=\"https://www.researchgate.net/profile/Christian_Bauckhage/publication/335099466/figure/fig1/AS:790344266444801@1565444165906/Contour-plot-of-the-objective-function-f-x-in-2-and-visualizations-of-the-two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "- Decision tree: SVMs learn a decision boundary that maximizes the distance between samples of the two classes, given a kernel. Decision trees learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referece\n",
    "- Christopher M. Bishop.\"Pattern Recognition and Machine Learning.\" Springer, 2006. [PDF](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
    "\n",
    "- [SVM in SVG](https://shogo82148.github.io/homepage/memo/algorithm/svm/kernel-svm.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
