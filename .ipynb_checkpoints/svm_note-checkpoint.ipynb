{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> *** Best viewed in jupyter notebook, not in GitHub* **</b>\n",
    "\n",
    "# Practical Introduction to Support Vector Machine\n",
    "\n",
    "- Hard-margin SVM\n",
    "  - Two-class linear classification\n",
    "  - Calculation of margin\n",
    "  - Langulange multiplier\n",
    "  - KKT condition (Support vectors)\n",
    "- Soft-margin SVM\n",
    "  - Slack variable\n",
    "  - Penalty control by C\n",
    "  - How to train SVM (SMO algorithm)\n",
    "- Nonlinear SVM\n",
    "  - Kernel function\n",
    "  - Kernel trick\n",
    "- Appendix\n",
    "  - Example of Langulange multipler\n",
    "  - Quadratic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathmatical fomulation for SVM decribed below mostly comes from the reference [1]. But, in order to make it easier to understand, I start with a simple linear kernel $\\phi(\\mathbf{x}) = \\mathbf{x}$ and wrote some extra notes.\n",
    "## 1. Hard-margin SVM\n",
    "\n",
    "### 1.1 Two-class linear classification\n",
    "\n",
    "We will derive the concept of support vector machine from the two-class classification using a linear model $y$ of the form \n",
    "\n",
    "\\begin{equation*}\n",
    "y(\\mathbf{x}) = \\mathbf{w}^T \\mathbf{x} + b\n",
    "\\label{eq:vector_ray} \\tag{1.1}\n",
    "\\end{equation*}\n",
    "\n",
    " where $\\bf{w}$ is a weight vector and $b$ is a bias parameter.\n",
    "\n",
    "The training data set comprises $N$ input vectors $\\mathbf{x}_1, ...,  \\mathbf{x}_N$ with\n",
    "corresponding target values $t_1,...,t_N$ where $t_n ∈ \\{−1, 1\\}$, and new data points $\\mathbf{x}$ are classified according to the sign of $y(\\mathbf{x})$.\n",
    "\n",
    "\n",
    "We shall assume for the moment that **the training data set is linearly separable in feature space**, so that by definition there exists at least one choice of the parameters\n",
    "$\\mathbf{w}$ and $b$ such that a function of the form (7.1) satisfies $y(\\mathbf{x}_n) > 0$ for points having\n",
    "$t_n = +1$ and $y(\\mathbf{x}_n) < 0$ for points having $t_n = −1$, so that $t_n y(\\mathbf{x}_n) > 0$ for all training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Calculation of margin\n",
    "In support vector machines the decision boundary is chosen to be the one for which the margin is maximized. For this purpose, we need to calculate the distance of a point $\\bf{x}_m$ to the decision surface (hyperplane)\n",
    "\n",
    "Consider two points $\\mathbf{x}_A$ and $\\mathbf{x}_B$ both of which lie on the decision surface.\n",
    "Because $y(\\mathbf{x}_A) = y(\\mathbf{x}_B) = 0$, we have $\\mathbf{w}^T (\\mathbf{x}_A − \\mathbf{x}_B)=0$ and hence the vector $\\mathbf{w}$ is\n",
    "orthogonal to every vector lying within the decision surface, and so $\\mathbf{w}$ determines the orientation of the decision surface. Then the point $\\mathbf{x}'$ on the hyperplane that is the closest point to the origin can be represented as $\\mathbf{x}' = a\\mathbf{w}$ where $a$ is a scalar value. Since $\\mathbf{x}'$ is on the hyperplane, we know that $\\mathbf{w}^T \\mathbf{x}' + b = 0\\Rightarrow a \\mathbf{w}^T \\mathbf{w} + b = 0 \\Rightarrow a = -\\frac{b}{||w||^2}$. Therefore, $||\\mathbf{x}'|| = ||aw|| = -\\frac{b}{||w||}$. Now rewrite $\\mathbf{x}'$ to $\\mathbf{x}$ which satisfy $y(\\mathbf{x}) = 0$, and the distance of a point $\\mathbf{x}$ from a hyperplane is given by $|y(\\mathbf{x})|/||w||$. Furthermore, we are only interested in solutions for which all data points are correctly classified, so that $t_n y(\\mathbf{x}_n) > 0$ for all $n$. Thus the distance of a point $\\mathbf{x}_n$ to the decision surface is given by \n",
    "\\begin{equation}\n",
    "\\frac{t_n y(\\mathbf{x}_n)}{||\\mathbf{w}||} = \\frac{t_n(\\mathbf{w}^T \\mathbf{x}_n + b)}{||\\mathbf{w}||}\n",
    "\\tag{1.2}\n",
    "\\end{equation}\n",
    "\n",
    "<img width=400 src=\"https://i.stack.imgur.com/tnAsd.png\">\n",
    "\n",
    "The margin is given by the perpendicular distance to the closest point $\\mathbf{x}_n$ from the\n",
    "data set, and we wish to optimize the parameters $\\mathbf{w}$ and $b$ in order to maximize this\n",
    "distance. Thus the maximum margin solution is found by solving\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{argmax}} \\{ \\frac{1}{||\\mathbf{w}||}\n",
    "\\underset{n}{\\mathrm{argmin}} [ t_n(\\mathbf{w}^T \\mathbf{x}_n + b) ] \\}\n",
    "\\end{equation}\n",
    "We note that the margin $|y(\\mathbf{x})|/||w||$ is invariant to the rescaling $\\mathbf{w} \\rightarrow k\\mathbf{w}$ and $b \\rightarrow kb$, and we use this freedom to set \n",
    "\\begin{equation}\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) = 1\n",
    "\\tag{1.3}\n",
    "\\end{equation}\n",
    "\n",
    "for the point that is closest to the surface. In this case, all data points ($n = 1, ..., N$) will satisfy the constraints\n",
    "\n",
    "\\begin{equation}\n",
    "t_n(\\mathbf{w}^T \\mathbf{x}_n + b) \\ge 1\n",
    "\\tag{1.4}\n",
    "\\end{equation}\n",
    "\n",
    "The optimization problem then simply requires that we maximize $\\frac{1}{||\\mathbf{w}||}$, which is\n",
    "equivalent to minimizing $||\\mathbf{w}||^2$, and so we have to solve the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{w}, b}{\\mathrm{argmin}}\n",
    "\\frac{1}{2}||\\mathbf{w}||^2\n",
    "\\tag{1.5}\n",
    "\\end{equation}\n",
    "\n",
    "subject to the constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Langulange multiplier\n",
    "In order to solve this constrained optimization problem, we introduce Lagrange multipliers $a_n \\ge 0$, with one multiplier for each of the constraints (See Appendix A), giving\n",
    "the Lagrangian function\n",
    "\n",
    "\\begin{equation}\n",
    "L(\\mathbf{w}, b, \\mathbf{a}) = \\frac{1}{2}||\\mathbf{w}||^2 − \n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n {t_n(\\mathbf{w}^T \\mathbf{x}_n + b) − 1}\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "Setting the derivatives of $L$ with respect to $\\bf{w}$ and $b$ equal to zero, we obtain the following two conditions\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \\overset{N}{\\underset{n=1}{\\sum}} a_n t_n \\mathbf{x}_n\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n t_n = 0\n",
    "\\tag{8}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Eliminating $\\bf{w}$ and $b$ from $L(\\mathbf{w}, b, \\mathbf{a})$ using these conditions, we obtain the objective function (dual represention) of the maximum margin problem in which we want to maximize\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m \\mathbf{x}_n^T \\mathbf{x}_m\n",
    "\\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "with respect to a subject to the constraints for $n = 1,...,N$\n",
    "\\begin{equation}\n",
    "a_n = 0 \\\\\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n t_n = 0.\n",
    "\\tag{10}\n",
    "\\end{equation}\n",
    "\n",
    "We shall see how to train the SVM to solve this **quadratic programming** problem (in dual form) later (Section 2.3).\n",
    "For classification of new data, we evaluate the sign\n",
    "of $y(\\mathbf{x})$ defined by (1). This can be expressed in terms of the parameters $\\{a_n\\}$ using (7) to give\n",
    "\\begin{equation}\n",
    "y(x) = \\overset{N}{\\underset{n=1}{\\sum}} a_n t_n \\mathbf{x}_n + b\n",
    "\\tag{11}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 KKT condition\n",
    "\n",
    "Because the above Lagrange multiplier was an optimazation of a function with inequality constraints, it has to satisfy the following condition, known as KTT condition (Karush–Kuhn–Tucker conditions).\n",
    "\n",
    "\\begin{equation}\n",
    "a_n \\ge 0 \\\\\n",
    "t_n y(x_n) − 1 \\ge 0 \\\\\n",
    "a_n \\{t_n y(x_n) − 1\\} = 0 \\\\\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "Here the idea of support vector comes in. For every data point, either $a_n = 0$ or $t_n y(\\mathbf{x}_n) = 1$. Any data point for\n",
    "which $a_n = 0$ will not appear in the sum in (7) and hence plays no role in making predictions for new data points. The remaining data points are called **support vectors**,\n",
    "and because they satisfy $t_n y(\\mathbf{x}_n) = 1$, they correspond to points that lie on the maximum margin hyperplanes in feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Soft-margin SVM\n",
    "\n",
    "### 2.1 Slack variable\n",
    "\n",
    "So far in hard-margin SVM, we have assumed that the training data points are linearly separable in the\n",
    "feature space $\\mathbf{x}$. This means every $\\mathbf{x}$ has to be outside of the margin $(-1 < y < 1)$. In practice, however, the class-conditional distributions\n",
    "may overlap, in which case exact separation of the training data can lead to poor\n",
    "generalization.\n",
    "We therefore need a way to modify the support vector machine so as to allow\n",
    "some of the training points to be misclassified.\n",
    "To do this, we introduce **slack variables**, $\\xi_n \\ge 0$ where\n",
    "$n = 1,...,N$, with one slack variable for each training data point.\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-1b899fcfd6dbee47d238b9b661aa2f05.webp\">\n",
    "\n",
    "These are defined by $\\xi_n = 0$ for data points that are on or\n",
    "inside the correct margin boundary and $\\xi_n = |t_n − y(\\mathbf{x}_n)|$ for other points.\n",
    "This can be fomulated using the hinge loss function \n",
    "\\begin{equation}\n",
    "\\xi_n = \\mathrm{max}(0, |t_n − y(\\mathbf{x}_n)|)),\n",
    "\\end{equation}\n",
    "Thus a\n",
    "data point that is on the decision boundary $y(\\mathbf{x}_n)=0$ will have $\\xi_n = 1$, \n",
    "and points with $\\xi_n > 1$ will be misclassified. The exact classification constraints (1.5) are then\n",
    "replaced with\n",
    "\\begin{equation}\n",
    "t_n y(\\mathbf{x}_n) \\ge 1 − \\xi_n\n",
    "\\end{equation}\n",
    "in which $n = 1,...,N$ and the slack variables are constrained to satisfy $\\xi_n \\ge 0$.\n",
    "\n",
    " Data points for which $\\xi_n = 0$ are correctly classified and are either on the margin or on the correct side\n",
    "of the margin. Points for which $0 \\lt \\xi_n \\le 1$ lie inside the margin, but on the correct side of the decision boundary, and those data points for which $\\xi_n \\gt 1$ lie on the wrong side of the decision boundary and are misclassified.\n",
    "\n",
    "The idea that assigning penalty to misclassified data and data inside the margin is sometimes described as relaxing the hard margin constraint to give a\n",
    "**soft margin** and allows some of the training set data points to be misclassified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Penalty control by C\n",
    "Our goal is now to maximize the margin while softly penalizing points that lie\n",
    "on the wrong side of the margin boundary. We therefore minimize\n",
    "\\begin{equation}\n",
    "C \\overset{N}{\\underset{n=1}{\\sum}} \\xi_n + \\frac{1}{2} ||\\mathbf{w}||^2\n",
    "\\end{equation}\n",
    "where the parameter $C \\gt 0$ controls the trade-off between the slack variable penalty\n",
    "and the margin. Because any point that is misclassified has $\\xi_n \\gt 1$, it follows that\n",
    "$\\Sigma_n^K \\xi_n$ is an upper bound on the number of misclassified points $K$. The parameter $C$ is\n",
    "therefore analogous to (the inverse of) a regularization coefficient because it controls\n",
    "the trade-off between minimizing training errors and controlling model complexity.\n",
    "In the limit $C \\rightarrow \\infty$, the objective function correspontds to the hard-margin SVM (if possible) which gives an $\\infty$ error for misclassified data and only works for linearly separable data.\n",
    "\n",
    "We now wish to minimize (7.21) subject to the constraints (7.20) together with\n",
    "$\\xi_n \\ge 0$. Interestingly, by following the same Langrange multipler approach for this optimazation problem, we will obtain the dual Lagrangian in the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m \\mathbf{x}_n^T \\mathbf{x}_m\n",
    "\\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "which is identical to (1.9) except the constraints (based on the KTT condition) are somehow different. Based on the results (7.32) and (7.33) in [1], we will have to minimize (7.32) with respect to the dual variables\n",
    "$\\{a_n\\}$ subject to\n",
    "\n",
    "\\begin{equation}\n",
    "0 \\le a_n \\le C \\\\\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n t_n = 0\n",
    "\\end{equation}\n",
    "for $n = 1,...,N$\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-2c3b599096d48b17a85640126f40d816\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 How to train SVM\n",
    "Sequential Minimal Optimization (SMO) is an iterative algorithm for solving the quadratic programming problem, populary employed during the training of SVM.\n",
    "\n",
    "Pseudcode of SMO algorithm is as follows:\n",
    "~~~\n",
    " INPUT (x_n, y_n) ∈ D\n",
    "OUTPUT a = (a_1, a_2, ..., a_N)^T\n",
    "-----------------------------------\n",
    "1: Initialize: a = 0\n",
    "2: WHILE any of a violates the KTT condition\n",
    "3:   Pick a_i which violates the KTT condition\n",
    "4:   Pick a_j\n",
    "5:   Update a_i and a_j in one step\n",
    "6: END WHILE\n",
    "7: RETURN a\n",
    "~~~\n",
    "\n",
    "We have to pick two variables because of the linear equality constraint (10). When SMO adjust one of the weights, it must also adjust another weight to keep the sum zero.\n",
    "To learn about the update of $a_i$ and $a_j$ to $a_i^{new}$ and $a_j^{new}$ in step 5, see [2].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nonlinear SVM\n",
    "\n",
    "### Kernel function\n",
    "So far, we did the linear classifion on the data $\\mathbf{x}$ simply in the data space $\\mathbf{x}$ because we assumed they are linearly separable (perhaps with a few noise in soft-margin SVM). \n",
    "But what if the data set just does not allow for the linear classification in the data space? One way to overcome this is to map the data $\\mathbf{x}$ to a (high-dimensional) feature space $\\phi(\\mathbf{x})$ so that we can find a better hyperplane which linearly separates the feature space. The example of this process is visualized in the video below. \n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=3liCbRZPrZA\">\n",
    "  <img src=\"https://img.youtube.com/vi/3liCbRZPrZA/0.jpg\" width=\"260\" height=180\" />\n",
    "</a>\n",
    "    \n",
    "    \n",
    "After transforming the data $\\mathbf{x} \\rightarrow \\phi(\\mathbf{x})$, the objective function of our linear classifier changes to\n",
    "\\begin{equation}\n",
    "\\overset{N}{\\underset{n=1}{\\sum}} a_n - \n",
    "\\frac{1}{2}\\overset{N}{\\underset{n=1}{\\sum}}\\overset{N}{\\underset{m=1}{\\sum}}\n",
    "a_n a_m t_n t_m k(\\mathbf{x_n}, \\mathbf{x_m})\n",
    "\\end{equation}\n",
    "\n",
    "where the **kernel function** is defined by $k(\\mathbf{x}, \\mathbf{x}') = \\phi(\\mathbf{x})^T \\phi(\\mathbf{x}')$. For our purpose, we can think the kernel function simply as the inner products in the feature space.\n",
    "\n",
    "Therefore, what we used in the previous section can be considered as the simplest kernel, the linear kernel $\\phi(\\mathbf{x}) = \\mathbf{x}$.\n",
    "\n",
    "### Kernel trick\n",
    "With kernel functions, you can operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the **kernel trick** and a class of algorithms which employs the technique is called kernel methods.\n",
    "\n",
    "The solution to a quadratic programming problem in M variables in general has\n",
    "computational complexity that is O(M3). In going to the dual formulation we have\n",
    "turned the original optimization problem, which involved minimizing (7.6) over M\n",
    "variables, into the dual problem (7.10), which has N variables. For a fixed set of\n",
    "basis functions whose number M is smaller than the number N of data points, the\n",
    "move to the dual problem appears disadvantageous. However, it allows the model to\n",
    "be reformulated using kernels, and so the maximum margin classifier can be applied\n",
    "efficiently to feature spaces whose dimensionality exceeds the number of data points,\n",
    "including infinite feature spaces. The kernel formulation also makes clear the role\n",
    "of the constraint that the kernel function k(x, x\u0004\n",
    ") be positive definite, because this\n",
    "ensures that the Lagrangian function L\u0004(a) is bounded below, giving rise to a welldefined optimization problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Example of Lagrange multiplier\n",
    "For problems where you want to find the maximum value of $f(x, y)$, subject to $g(x, y)$, you can use the following approach,\n",
    "> Given $L(x, y, \\lambda) = f(x, y) - \\lambda g(x, y)$, if $(\\alpha, \\beta)$ gives an extreme value, it is a solution of $\\frac{\\delta L}{\\delta x} = \\frac{\\delta L}{\\delta y} = \\frac{\\delta L}{\\delta \\lambda} = 0$ or $\\frac{\\delta g}{\\delta x} = \\frac{\\delta g}{\\delta y} = 0$\n",
    "\n",
    "This is called Lagrange multiplier and $L$ is called Lagrange function. A simple example follows:\n",
    "\n",
    "> Find the maximum value of the objective function $f(x, y) = 2x + 3y$, subject to the constraint $x^2 + y^2 = 1$\n",
    "\n",
    "In this case, we have a constraint $g(x, y) = x^2 + y^2 - 1 = 0$.\n",
    "First, we write $L$ with three variables.\n",
    "$L(x,y,\\lambda)=2x+3y−\\lambda(x^2+y^2−1)$\n",
    "\n",
    "Setting the derivatives of $L(x, y, \\lambda)$ with respect to each variable to zero, \n",
    "\\begin{equation}\n",
    "\\frac{\\delta L}{\\delta x} = 2 - 2x\\lambda = 0\\\\\n",
    "\\frac{\\delta L}{\\delta y} = 3 - 2y\\lambda = 0\\\\\n",
    "\\frac{\\delta L}{\\delta \\lambda} = -x^2 - y^2 + 1 = 0\n",
    "\\end{equation}\n",
    "\n",
    "we obtain $x = 23y$ by removing $\\lambda$ from the equations.\n",
    "Use this in the third equation, then the candidate positions of extreme values are\n",
    "$(x,y)=(\\pm \\frac{2}{\\sqrt{13}}, \\pm\\frac{3}{\\sqrt{13}})$ (See the code below)\n",
    "\n",
    "Good news is that you can apply Lagrange multiplier to any other multivariable functions with more than two variables ($x, y$). Also, more importantly, multiple constraints can be dealt in the same way. If you have two constraints $g(x) = 0$, $h(x) = 0$, we obtain the Lagrange function $L = f - \\lambda g - \\mu h$ using Lagrange multipliers $\\lambda$ and $\\mu$. This is what we see in the SVM part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x113ec8c18>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "kp = 2*2/13**0.5 + 3*3/13**0.5\n",
    "km = -2*2/13**0.5 - 3*3/13**0.5\n",
    "y = lambda x, k: -(2/3)*x + k/3\n",
    "xs = np.linspace(-2, 2, 50)\n",
    "circle = plt.Circle((0, 0), 1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.add_artist(circle)\n",
    "ax.plot(xs, [y(x, kp) for x in xs], c=\"r\", label=\"max f(x, y)\")\n",
    "ax.plot(xs, [y(x, km) for x in xs], c=\"c\", label=\"min f(x, y)\")\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_title(\"What did happen in the example?\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Quadratic programming\n",
    "Quadratic programming optimizes (minimizing or maximizing) a quadratic function of several variables subject to linear constraints on these variables. Intuitively, in case of N = 2, this is similar to find an optimal countor group of an ellipse/circle or other quadratic shape with several linear constraints. <img width=300 src=\"https://www.researchgate.net/profile/Christian_Bauckhage/publication/335099466/figure/fig1/AS:790344266444801@1565444165906/Contour-plot-of-the-objective-function-f-x-in-2-and-visualizations-of-the-two.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "- Multiclass SVM [Bishop] Section 7.1.3\n",
    "- SVM for regression [Bishop] Section 7.1.4\n",
    "- Decision tree: SVMs learn a decision boundary that maximizes the distance between samples of the two classes, given a kernel. Decision trees learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).\n",
    "- $\\nu$-SVM: An alternative, equivalent formulation of the support vector machine proposed by Scholkopf et al. (2000). [Bishop] P334~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referece\n",
    "[1] Christopher M. Bishop.\"Pattern Recognition and Machine Learning.\" Springer, 2006. [PDF](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf)\n",
    "\n",
    "[2] http://pages.cs.wisc.edu/~dpage/cs760/SMOlecture.pdf\n",
    "\n",
    "[3] [SVM in SVG](https://shogo82148.github.io/homepage/memo/algorithm/svm/kernel-svm.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
